{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import re\n",
    "import textwrap\n",
    "\n",
    "import tiktoken\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.retrievers import TFIDFRetriever\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Retrieval and chains\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "def num_tokens(text: str) -> int:\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "# Inisialisasi model embedding\n",
    "openai_embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "# nomic_embeddings = OllamaEmbeddings(\n",
    "#     model=\"nomic-embed-text\"\n",
    "# )\n",
    "# # Inisialisasi IndoBERT sebagai embedding\n",
    "# indobert_embeddings = HuggingFaceEmbeddings(\n",
    "#     model_name=\"indobenchmark/indobert-base-p1\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(pdf_path):\n",
    "    \"\"\"Load dan ekstrak teks dari PDF\"\"\"\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    pages = loader.load()\n",
    "    \n",
    "    # Gabungkan semua halaman menjadi satu teks\n",
    "    full_text = \"\"\n",
    "    for page in pages:\n",
    "        full_text += page.page_content + \"\\n\"\n",
    "    \n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Membersihkan teks dengan aturan khusus untuk BAB dan Pasal,\n",
    "    dan tidak menambahkan newline pada 'Pasal x' yang didahului kata 'dalam' atau 'dan'.\"\"\"\n",
    "\n",
    "    # Potong teks mulai dari BAB I jika ada\n",
    "    match = re.search(r'\\bBAB\\s+I\\b', text, flags=re.IGNORECASE)\n",
    "    if match:\n",
    "        text = text[match.start():]\n",
    "\n",
    "    # Hapus header tidak perlu\n",
    "    text = re.sub(r'PRESIDEN\\s+REPUBLIK\\s+INDONESIA', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'-\\s*\\d+\\s*-', '', text)\n",
    "\n",
    "    # Hapus semua baris yang mengandung '...' atau karakter elipsis Unicode '…'\n",
    "    text = re.sub(r'^.*(\\.{3}|…).*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Hapus baris kosong sisa\n",
    "    text = re.sub(r'^\\s*\\n', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Normalisasi spasi jadi satu spasi\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Hapus baris seperti 'Pasal x Cukup jelas'\n",
    "    text = re.sub(r'\\bPasal\\s+\\d+\\s+Cukup jelas\\b', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Tambahkan newline sebelum BAB (angka romawi)\n",
    "    text = re.sub(r'(?<!\\n)(BAB\\s+[IVXLCDM]+)', r'\\n\\1', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Tambahkan newline sebelum 'Pasal x' yang berdiri sendiri\n",
    "    text = re.sub(\n",
    "        r'(?<!\\S)(Pasal\\s+\\d+)\\b(?!\\s*(ayat\\b|,|dan\\b))',\n",
    "        r'\\n\\1',\n",
    "        text,\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    # Hilangkan newline pada 'dalam\\nPasal x' dan 'dan\\nPasal x'\n",
    "    text = re.sub(r'(dalam|dan)\\s*\\n\\s*(Pasal\\s+\\d+)', r'\\1 \\2', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Bersihkan spasi di awal dan akhir\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uji fungsi load_pdf dengan path contoh\n",
    "pdf_path = \"data/UU Nomor 13 Tahun 2003.pdf\"\n",
    "try:\n",
    "    # Ekstrak sampel teks (50 karakter awal dan akhir) untuk ditampilkan\n",
    "    raw_text = load_pdf(pdf_path)\n",
    "    with open(\"data/raw_text_uu_no_13_2023_1.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(raw_text)\n",
    "    # print(f\"Contoh data awal raw_text: {raw_text[:100000]}...\")\n",
    "    preprocessed_text = preprocess_text(raw_text)\n",
    "    with open(\"data/raw_text_uu_no_13_20232.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(preprocessed_text)\n",
    "    print(f\"Berhasil memuat PDF! Total karakter: {len(preprocessed_text)}\")\n",
    "    print(f\"Sample awal teks:\\n{preprocessed_text[:100000]}...\")\n",
    "    print(f\"Sample akhir teks:\\n...{preprocessed_text[-100:]}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File tidak ditemukan: {pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_uu_with_recursive_splitter(text: str, min_words: int = 400, max_words: int = 800, overlap: int = 50):\n",
    "    # text = preprocess_text(text)\n",
    "\n",
    "    max_chars = max_words * 6\n",
    "    overlap_chars = overlap * 6\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=max_chars,\n",
    "        chunk_overlap=overlap_chars,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    bab_pattern = re.compile(r'^\\s*(BAB\\s+([IVXLCDM]+)\\s+([A-Z\\s,/.&-]+))\\s*$', re.MULTILINE)\n",
    "    bab_matches = list(bab_pattern.finditer(text))\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    for i, match in enumerate(bab_matches):\n",
    "        start = match.end()\n",
    "        end = bab_matches[i+1].start() if i+1 < len(bab_matches) else len(text)\n",
    "\n",
    "        bab_nomor = f\"BAB {match.group(2)}\"\n",
    "        bab_judul = match.group(3).strip()\n",
    "\n",
    "        bab_content = text[start:end].strip()\n",
    "\n",
    "        # Split per pasal\n",
    "        pasal_pattern = re.compile(r'(?=\\nPasal\\s+\\d+\\s+)')\n",
    "        pasal_texts = pasal_pattern.split(bab_content)\n",
    "\n",
    "        for j, pasal_text in enumerate(pasal_texts):\n",
    "            if not pasal_text.strip():\n",
    "                continue\n",
    "\n",
    "            pasal_match = re.search(r'Pasal\\s+(\\d+)', pasal_text)\n",
    "            pasal_nomor = f\"Pasal {pasal_match.group(1)}\" if pasal_match else f\"Bagian {j}\"\n",
    "\n",
    "            word_count = len(pasal_text.split())\n",
    "\n",
    "            # Jika teks panjang, bagi menjadi beberapa chunk\n",
    "            if word_count > max_words:\n",
    "                chunks = text_splitter.create_documents([pasal_text])\n",
    "                for k, chunk in enumerate(chunks):\n",
    "                    chunk.page_content = f\"{bab_nomor} {bab_judul} - {pasal_nomor} : {chunk.page_content.strip()}\"\n",
    "                    chunk.metadata = {\n",
    "                        \"bab_nomor\": bab_nomor,\n",
    "                        \"bab_judul\": bab_judul,\n",
    "                        \"pasal_nomor\": pasal_nomor,\n",
    "                        \"chunk\": f\"{k+1}/{len(chunks)}\",\n",
    "                        \"source\": \"UU No. 13 Tahun 2003\",\n",
    "                        \"word_count\": len(chunk.page_content.split()),\n",
    "                        \"full_reference\": f\"{bab_nomor} {bab_judul} - {pasal_nomor} (Bagian {k+1}/{len(chunks)})\"\n",
    "                    }\n",
    "                    documents.append(chunk)\n",
    "            else:\n",
    "                page_text = f\"{bab_nomor} {bab_judul} - {pasal_nomor} : {pasal_text.strip()}\"\n",
    "                doc = Document(\n",
    "                    page_content=page_text,\n",
    "                    metadata={\n",
    "                        \"bab_nomor\": bab_nomor,\n",
    "                        \"bab_judul\": bab_judul,\n",
    "                        \"pasal_nomor\": pasal_nomor,\n",
    "                        \"chunk\": \"1/1\",\n",
    "                        \"source\": \"UU No. 13 Tahun 2003\",\n",
    "                        \"word_count\": word_count,\n",
    "                        \"full_reference\": f\"{bab_nomor} {bab_judul} - {pasal_nomor}\"\n",
    "                    }\n",
    "                )\n",
    "                documents.append(doc)\n",
    "\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_uu_by_combined_pasal(\n",
    "    text: str, min_words: int = 300, max_words: int = 800, overlap: int = 50\n",
    "):\n",
    "    # text = preprocess_text(text)\n",
    "\n",
    "    # Estimasi jumlah karakter berdasarkan jumlah kata (asumsi 6 huruf per kata)\n",
    "    max_chars = max_words * 6\n",
    "    overlap_chars = overlap * 6\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=max_chars,\n",
    "        chunk_overlap=overlap_chars,\n",
    "        length_function=len,\n",
    "        separators=[\". \", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    # Temukan semua BAB\n",
    "    bab_pattern = re.compile(r'^\\s*(BAB\\s+([IVXLCDM]+)\\s+([A-Z\\s,/.&-]+))\\s*$', re.MULTILINE)\n",
    "    bab_matches = list(bab_pattern.finditer(text))\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    for i, match in enumerate(bab_matches):\n",
    "        start = match.end()\n",
    "        end = bab_matches[i + 1].start() if i + 1 < len(bab_matches) else len(text)\n",
    "\n",
    "        bab_nomor = f\"BAB {match.group(2)}\"\n",
    "        bab_judul = match.group(3).strip()\n",
    "        bab_content = text[start:end].strip()\n",
    "\n",
    "        # Split berdasarkan awal Pasal\n",
    "        pasal_pattern = re.compile(r'(?:^|\\n)(?=Pasal\\s+\\d+\\b)')\n",
    "\n",
    "        pasal_texts = pasal_pattern.split(bab_content)\n",
    "\n",
    "        buffer_text = \"\"\n",
    "        buffer_pasal = []\n",
    "\n",
    "        for j, pasal_text in enumerate(pasal_texts):\n",
    "            pasal_text = pasal_text.strip()\n",
    "            if not pasal_text:\n",
    "                continue\n",
    "\n",
    "            # Coba ekstrak nomor pasal\n",
    "            pasal_match = re.search(r'Pasal\\s+(\\d+)', pasal_text)\n",
    "            pasal_nomor = f\"Pasal {pasal_match.group(1)}\" if pasal_match else f\"Pasal-{j+1}\"\n",
    "\n",
    "            buffer_text += \"\\n\" + pasal_text\n",
    "            buffer_pasal.append(pasal_nomor)\n",
    "\n",
    "            word_count = len(buffer_text.split())\n",
    "\n",
    "            # Jika buffer cukup panjang atau sudah di akhir, simpan chunk\n",
    "            if word_count >= min_words or j == len(pasal_texts) - 1:\n",
    "                page_text = f\"{bab_nomor} {bab_judul} :\\n{buffer_text.strip()}\".lower()\n",
    "\n",
    "                if len(page_text.split()) > max_words:\n",
    "                    chunks = text_splitter.create_documents([page_text])\n",
    "                    for k, chunk in enumerate(chunks):\n",
    "                        # Sisipkan kembali informasi BAB & Pasal di awal isi\n",
    "                        if k != 0:\n",
    "                            new_content = f\"{bab_nomor} {bab_judul} :\\n{', '.join(buffer_pasal)} {chunk.page_content.strip()}\".lower()\n",
    "                        else:\n",
    "                            new_content = chunk.page_content.strip()\n",
    "                        chunk.page_content = new_content\n",
    "                        # chunk.page_content = chunk.page_content.strip()\n",
    "                        chunk.metadata = {\n",
    "                            \"bab_nomor\": bab_nomor,\n",
    "                            \"bab_judul\": bab_judul,\n",
    "                            \"pasal_nomor\": \", \".join(buffer_pasal),\n",
    "                            \"chunk\": f\"{k+1}/{len(chunks)}\",\n",
    "                            \"source\": \"UU No. 13 Tahun 2003\",\n",
    "                            \"word_count\": len(chunk.page_content.split()),\n",
    "                            \"full_reference\": f\"{bab_nomor} {bab_judul} - {', '.join(buffer_pasal)} (Bagian {k+1}/{len(chunks)})\"\n",
    "                        }\n",
    "                        documents.append(chunk)\n",
    "                else:\n",
    "                    doc = Document(\n",
    "                        page_content=page_text.strip(),\n",
    "                        metadata={\n",
    "                            \"bab_nomor\": bab_nomor,\n",
    "                            \"bab_judul\": bab_judul,\n",
    "                            \"pasal_nomor\": \", \".join(buffer_pasal),\n",
    "                            \"chunk\": \"1/1\",\n",
    "                            \"source\": \"UU No. 13 Tahun 2003\",\n",
    "                            \"word_count\": word_count,\n",
    "                            \"full_reference\": f\"{bab_nomor} {bab_judul} - {', '.join(buffer_pasal)}\"\n",
    "                        }\n",
    "                    )\n",
    "                    documents.append(doc)\n",
    "\n",
    "                buffer_text = \"\"\n",
    "                buffer_pasal = []\n",
    "\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_uu_by_combined_pasal(\n",
    "    text: str, min_words: int = 300, max_words: int = 800, overlap: int = 50\n",
    "):\n",
    "    # text = preprocess_text(text)\n",
    "\n",
    "    # Estimasi jumlah karakter berdasarkan jumlah kata (asumsi 6 huruf per kata)\n",
    "    max_chars = max_words * 6\n",
    "    overlap_chars = overlap * 6\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=max_chars,\n",
    "        chunk_overlap=overlap_chars,\n",
    "        length_function=len,\n",
    "        separators=[\". \", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    # Temukan semua BAB\n",
    "    bab_pattern = re.compile(r'^\\s*(BAB\\s+([IVXLCDM]+)\\s+([A-Z\\s,/.&-]+))\\s*$', re.MULTILINE)\n",
    "    bab_matches = list(bab_pattern.finditer(text))\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    for i, match in enumerate(bab_matches):\n",
    "        start = match.end()\n",
    "        end = bab_matches[i + 1].start() if i + 1 < len(bab_matches) else len(text)\n",
    "\n",
    "        bab_nomor = f\"BAB {match.group(2)}\"\n",
    "        bab_judul = match.group(3).strip()\n",
    "        bab_content = text[start:end].strip()\n",
    "\n",
    "        # Split berdasarkan awal Pasal\n",
    "        pasal_pattern = re.compile(r'(?:^|\\n)(?=Pasal\\s+\\d+\\b)')\n",
    "\n",
    "        pasal_texts = pasal_pattern.split(bab_content)\n",
    "\n",
    "        buffer_text = \"\"\n",
    "        buffer_pasal = []\n",
    "\n",
    "        for j, pasal_text in enumerate(pasal_texts):\n",
    "            pasal_text = pasal_text.strip()\n",
    "            if not pasal_text:\n",
    "                continue\n",
    "\n",
    "            # Coba ekstrak nomor pasal\n",
    "            pasal_match = re.search(r'Pasal\\s+(\\d+)', pasal_text)\n",
    "            pasal_nomor = f\"Pasal {pasal_match.group(1)}\" if pasal_match else f\"Pasal-{j+1}\"\n",
    "\n",
    "            buffer_text += \"\\n\" + pasal_text\n",
    "            buffer_pasal.append(pasal_nomor)\n",
    "\n",
    "            word_count = len(buffer_text.split())\n",
    "\n",
    "            # Jika buffer cukup panjang atau sudah di akhir, simpan chunk\n",
    "            if word_count >= min_words or j == len(pasal_texts) - 1:\n",
    "                page_text = f\"{bab_nomor} {bab_judul} :\\n{buffer_text.strip()}\".lower()\n",
    "\n",
    "                if len(page_text.split()) > max_words:\n",
    "                    chunks = text_splitter.create_documents([page_text])\n",
    "                    for k, chunk in enumerate(chunks):\n",
    "                        # Sisipkan kembali informasi BAB & Pasal di awal isi\n",
    "                        if k != 0:\n",
    "                            new_content = f\"{bab_nomor} {bab_judul} :\\n{', '.join(buffer_pasal)} {chunk.page_content.strip()}\".lower()\n",
    "                        else:\n",
    "                            new_content = chunk.page_content.strip()\n",
    "                        chunk.page_content = new_content\n",
    "                        # chunk.page_content = chunk.page_content.strip()\n",
    "                        chunk.metadata = {\n",
    "                            \"bab_nomor\": bab_nomor,\n",
    "                            \"bab_judul\": bab_judul,\n",
    "                            \"pasal_nomor\": \", \".join(buffer_pasal),\n",
    "                            \"chunk\": f\"{k+1}/{len(chunks)}\",\n",
    "                            \"source\": \"UU No. 13 Tahun 2003\",\n",
    "                            \"word_count\": len(chunk.page_content.split()),\n",
    "                            \"full_reference\": f\"{bab_nomor} {bab_judul} - {', '.join(buffer_pasal)} (Bagian {k+1}/{len(chunks)})\"\n",
    "                        }\n",
    "                        documents.append(chunk)\n",
    "                else:\n",
    "                    doc = Document(\n",
    "                        page_content=page_text.strip(),\n",
    "                        metadata={\n",
    "                            \"bab_nomor\": bab_nomor,\n",
    "                            \"bab_judul\": bab_judul,\n",
    "                            \"pasal_nomor\": \", \".join(buffer_pasal),\n",
    "                            \"chunk\": \"1/1\",\n",
    "                            \"source\": \"UU No. 13 Tahun 2003\",\n",
    "                            \"word_count\": word_count,\n",
    "                            \"full_reference\": f\"{bab_nomor} {bab_judul} - {', '.join(buffer_pasal)}\"\n",
    "                        }\n",
    "                    )\n",
    "                    documents.append(doc)\n",
    "\n",
    "                buffer_text = \"\"\n",
    "                buffer_pasal = []\n",
    "\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"data\\raw_text_uu_no_13_2023_amandemen.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "documents = chunk_uu_by_combined_pasal(text)\n",
    "print(f\"Total dokumen yang dihasilkan: {len(documents)}\")\n",
    "\n",
    "for doc in documents[:100]:\n",
    "    print(f\"Dokumen:\")\n",
    "    print(doc.page_content)\n",
    "    print(f\"Metadata:\")\n",
    "    print(textwrap.fill(str(doc.metadata), width=100))\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(documents, embedding_model,  persist_dir: str = \"./chroma_db\") -> Chroma: \n",
    "    # Create vector store\n",
    "    db = Chroma.from_documents(\n",
    "        documents=documents, \n",
    "        embedding= embedding_model,\n",
    "        persist_directory=persist_dir,\n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "    \n",
    "    # Persist to disk\n",
    "    # db.persist()\n",
    "    print(f\"Vector store saved to {persist_dir}\")\n",
    "    \n",
    "    return db\n",
    "\n",
    "db = create_vector_store(documents,embedding_model=openai_embeddings,  persist_dir=\"./chroma_db_openai_gabungan_pasal_cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_documents(query: str, persist_dir: str, embedding_model, top_k: int = 5):\n",
    "    db = Chroma(\n",
    "        persist_directory=persist_dir,\n",
    "        embedding_function=embedding_model\n",
    "    ) \n",
    "    result_with_score = db.similarity_search_with_score(\n",
    "        query, \n",
    "        k=top_k, \n",
    "    )\n",
    "\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    print([score for _, score in result_with_score])\n",
    "    print(\"Hasil pencarian:\")\n",
    "    for i, (doc, score) in enumerate(result_with_score):\n",
    "        print(f\"{i+1}.\")\n",
    "        print(f\"Skor Similarity : {score:.4f}\")\n",
    "        print(f\"Isi Dokumen     : {doc.page_content}\\n\")\n",
    "    \n",
    "    return result_with_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query =  \"apa hak pekerja?\"\n",
    "search_documents(query, \"./chroma_db_openai_gabungan_pasal_cosine\", openai_embeddings, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hybrid_retriever(\n",
    "    embedding_model,\n",
    "    tfidf_k: int = 2,\n",
    "    embed_k: int = 2,\n",
    "    weights: list[float] = [0.7, 0.3],\n",
    "    persist_directory: str = \"./chroma_db_nomic\"\n",
    "):\n",
    "    # Inisialisasi Chroma vectorstore\n",
    "    db = Chroma(\n",
    "        persist_directory=persist_directory,\n",
    "        embedding_function=embedding_model\n",
    "    )\n",
    "\n",
    "    # Ambil semua dokumen dari Chroma\n",
    "    raw_docs = db.get(include=[\"documents\", \"metadatas\"])\n",
    "    documents = [\n",
    "        Document(page_content=doc, metadata=meta)\n",
    "        for doc, meta in zip(raw_docs[\"documents\"], raw_docs[\"metadatas\"])\n",
    "    ]\n",
    "\n",
    "    # Buat TF-IDF retriever dari dokumen\n",
    "    tfidf_retriever = TFIDFRetriever.from_documents(documents)\n",
    "    tfidf_retriever.k = tfidf_k\n",
    "\n",
    "    # Buat retriever berbasis embedding dari Chroma\n",
    "    embedding_retriever = db.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": embed_k}\n",
    "    )\n",
    "\n",
    "    # Gabungkan keduanya dengan EnsembleRetriever\n",
    "    hybrid_retriever = EnsembleRetriever(\n",
    "        retrievers=[embedding_retriever, tfidf_retriever],\n",
    "        weights=weights\n",
    "    )\n",
    "\n",
    "    return hybrid_retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retriever_context(input_query):\n",
    "    retriever = build_hybrid_retriever(embedding_model=openai_embeddings,persist_directory=\"./chroma_db_openai_gabungan_pasal_cosine\")\n",
    "    documents = retriever.invoke(input_query)\n",
    "    chunks = [doc.page_content for doc in documents] if documents else []\n",
    "    context = \"\\n\\n\".join(chunks) if chunks else \"Tidak ada dokumen relevan ditemukan.\"\n",
    "    return {\n",
    "        \"context\": context,\n",
    "        \"chunks\": chunks,\n",
    "        \"metadata\": [doc.metadata for doc in documents],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_retriever = build_hybrid_retriever(embedding_model=openai_embeddings, persist_directory=\"./chroma_db_openai_gabungan_pasal_cosine\")\n",
    "docs = hybrid_retriever.get_relevant_documents(query)\n",
    "\n",
    "print(f\"\\nHasil Retrieve untuk pertanyaan: '{query}'\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"\\nDokumen #{i}\")\n",
    "    print(\"-\" * 30)\n",
    "    print(doc.page_content)  # isi dokumen\n",
    "    if doc.metadata:\n",
    "        print(\"\\n Metadata:\")\n",
    "        for key, value in doc.metadata.items():\n",
    "            print(f\"  - {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template prompt \n",
    "prompt_template = \"\"\"<|system|>\n",
    "Kamu adalah asisten hukum yang ahli dalam regulasi ketenagakerjaan di Indonesia, khususnya Undang-Undang Nomor 13 Tahun 2003 tentang Ketenagakerjaan beserta perubahannya, termasuk dari UU No. 6 Tahun 2023 (Cipta Kerja). \n",
    "Tugasmu adalah menjawab pertanyaan pengguna secara akurat dan objektif, berdasarkan kutipan resmi dari Undang-Undang yang tersedia dalam konteks.\n",
    "\n",
    "Aturan penjawaban:\n",
    "- Jawab secara singkat, fokus pada pasal dan ayat yang relevan.\n",
    "- jawab dengan format yang jelas, sebutkan pasal dan ayatnya.\n",
    "- Jika ada nomor dan list, tolong gunakan format yang sesuai.\n",
    "- Jawaban hanya boleh berdasarkan konteks yang tersedia.\n",
    "- Jika pasal dalam konteks telah diubah, tambahkan keterangan bahwa isi tersebut merupakan hasil amandemen dan sebutkan UU yang mengubahnya.\n",
    "- Jika pasal telah dihapus, sebutkan bahwa pasal tersebut sudah tidak berlaku dan tidak perlu dijelaskan lebih lanjut.\n",
    "- Sebutkan pasal dan ayat secara eksplisit jika tersedia.\n",
    "- Jangan menambahkan interpretasi atau opini di luar kutipan.\n",
    "- Jika tidak ditemukan informasi yang relevan dalam konteks, jawab: \"Informasi tidak tersedia dalam konteks.\"\n",
    "\n",
    "Format konteks:\n",
    "- Diawali dengan nama **Bab** (jika ada) dan **Nomor Pasal**, misalnya `bab ix hubungan kerja - pasal 50`.\n",
    "- Diikuti isi pasal dan ayat, termasuk penanda seperti `(1)`, `(2)`, dst.\n",
    "- Pasal yang diamandemen, ditambahkan, atau dihapus memiliki catatan tambahan dalam teks, misalnya:\n",
    "    - `Pasal 151 (diubah oleh UU No. 6 Tahun 2023)`\n",
    "    - `Pasal 151A (ditambahkan oleh UU No. 6 Tahun 2023)`\n",
    "    - `Pasal 152 (dihapus oleh UU No. 6 Tahun 2023)`\n",
    "\n",
    "Instruksi penting:\n",
    "- Jika pasal merupakan hasil perubahan dari UU lain, beri tahu pengguna bahwa pasal tersebut adalah hasil amandemen dari pasal sebelumnya.\n",
    "- Jika ada pasal baru (misal 151A), nyatakan bahwa pasal tersebut merupakan tambahan dari UU perubahan.\n",
    "- Jika pasal dihapus, cukup nyatakan bahwa pasal tersebut sudah tidak berlaku.\n",
    "\n",
    "<|user|>\n",
    "Berikut adalah konteks dari dokumen hukum yang relevan (kutipan dari UU No. 13 Tahun 2003 dan perubahannya):\n",
    "\n",
    "{context}\n",
    "\n",
    "Pertanyaan:\n",
    "{question}\n",
    "<|assistant|>\n",
    "/no_think\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi LLM dari Ollama\n",
    "llm = OllamaLLM(\n",
    "    model=\"qwen3:8b\",\n",
    "    temperature=0.0,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Ambil konteks dari retriever untuk query yang diberikan\n",
    "retrieved_context = get_retriever_context(query)\n",
    "context_text = retrieved_context[\"context\"]\n",
    "retrieved_chunks = retrieved_context[\"chunks\"]\n",
    "\n",
    "# Bangun prompt dan jalankan inferensi\n",
    "prompt_template = PromptTemplate(\n",
    "    template=prompt_template,  \n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa_chain = prompt_template | llm | StrOutputParser()\n",
    "answer = qa_chain.invoke({\n",
    "    \"context\": context_text,\n",
    "    \"question\": query\n",
    "})\n",
    "\n",
    "# Tampilkan hasil\n",
    "print(\"Jawaban LLM:\\n\", answer)\n",
    "print(\"\\nChunks yang digunakan sebagai konteks:\")\n",
    "for idx, chunk in enumerate(retrieved_chunks, 1):\n",
    "    print(f\"\\nChunk {idx}:\\n{chunk}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
