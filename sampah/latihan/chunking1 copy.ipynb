{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Libraries - Updated with latest LangChain packages\n",
    "import re\n",
    "import os\n",
    "import textwrap\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Document loading and processing\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "# Vector store and embeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# LLM integration\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Retrieval and chains\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masukkan OpenAI API key Anda\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ekstraksi Teks dari PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(pdf_path):\n",
    "    \"\"\"Load dan ekstrak teks dari PDF\"\"\"\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    pages = loader.load()\n",
    "    \n",
    "    # Gabungkan semua halaman menjadi satu teks\n",
    "    full_text = \"\"\n",
    "    for page in pages:\n",
    "        full_text += page.page_content + \"\\n\"\n",
    "    \n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Membersihkan dan mempersiapkan teks untuk chunking\"\"\"\n",
    "    # Hapus header dan footer yang tidak diperlukan\n",
    "    text = re.sub(r'PRESIDEN\\s+REPUBLIK\\s+INDONESIA', '', text)\n",
    "    text = re.sub(r'-\\s*\\d+\\s*-', '', text)\n",
    "    \n",
    "    # Normalisasi spasi dan line breaks\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Pastikan ada baris baru sebelum BAB dan Pasal untuk memudahkan pendeteksian\n",
    "    text = re.sub(r'(?<!\\n)(BAB\\s+[IVXLCDM]+)', r'\\n\\1', text)\n",
    "    text = re.sub(r'(?<!\\n)(Pasal\\s+\\d+)', r'\\n\\1', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uji fungsi load_pdf dengan path contoh\n",
    "pdf_path = \"../data/UU Nomor 13 Tahun 2003.pdf\"\n",
    "try:\n",
    "    # Ekstrak sampel teks (50 karakter awal dan akhir) untuk ditampilkan\n",
    "    raw_text = load_pdf(pdf_path)\n",
    "    preprocessed_text = preprocess_text(raw_text)\n",
    "    print(f\"Berhasil memuat PDF! Total karakter: {len(preprocessed_text)}\")\n",
    "    print(f\"Sample awal teks:\\n{preprocessed_text[:100]}...\")\n",
    "    print(f\"Sample akhir teks:\\n...{preprocessed_text[-100:]}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File tidak ditemukan: {pdf_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fungsi Ekstraksi Struktur UU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_uu_with_recursive_splitter(text: str, min_words: int = 400, max_words: int = 800, overlap: int = 50):\n",
    "    \"\"\"\n",
    "    Split UU text using RecursiveCharacterTextSplitter while preserving pasal structure as much as possible.\n",
    "    \n",
    "    Args:\n",
    "        text (str): UU text to be split\n",
    "        min_words (int): Minimum words per chunk\n",
    "        max_words (int): Maximum words per chunk\n",
    "        overlap (int): Number of overlapping words between chunks\n",
    "    \n",
    "    Returns:\n",
    "        List[Document]: List of document chunks\n",
    "    \"\"\"\n",
    "    # Preprocess text\n",
    "    text = preprocess_text(text)\n",
    "    \n",
    "    # Create splitter for pasal division\n",
    "    pasal_splitter = re.compile(r'(?=\\nPasal\\s+\\d+\\s+)')\n",
    "    pasal_texts = pasal_splitter.split(text)\n",
    "    \n",
    "    # Create splitter for text that's too long\n",
    "    # Convert word count to character count (estimate: 1 word = 6 characters)\n",
    "    min_chars = min_words * 6\n",
    "    max_chars = max_words * 6\n",
    "    overlap_chars = overlap * 6\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=max_chars,\n",
    "        chunk_overlap=overlap_chars,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    documents = []\n",
    "    current_bab = \"BAB I\"\n",
    "    current_bab_title = \"KETENTUAN UMUM\"  # Default title\n",
    "    \n",
    "    # BAB pattern detection\n",
    "    bab_pattern = re.compile(r'BAB\\s+([IVXLCDM]+)\\s+([A-Z][A-Z\\s,/.&-]+)')\n",
    "    \n",
    "    for i, pasal_text in enumerate(pasal_texts):\n",
    "        # Skip empty sections\n",
    "        if not pasal_text.strip():\n",
    "            continue\n",
    "        \n",
    "        # Check if there's new BAB information\n",
    "        bab_match = bab_pattern.search(pasal_text)\n",
    "        if bab_match:\n",
    "            current_bab = f\"BAB {bab_match.group(1)}\"\n",
    "            current_bab_title = bab_match.group(2).strip()\n",
    "            # If this is just a BAB header, skip\n",
    "            if len(pasal_text.strip()) < 100 and not re.search(r'Pasal\\s+\\d+', pasal_text):\n",
    "                continue\n",
    "        \n",
    "        # Extract pasal number if available\n",
    "        pasal_num_match = re.search(r'Pasal\\s+(\\d+)', pasal_text)\n",
    "        pasal_nomor = f\"Pasal {pasal_num_match.group(1)}\" if pasal_num_match else f\"Bagian {i}\"\n",
    "        \n",
    "        # Count words\n",
    "        word_count = len(pasal_text.split())\n",
    "        \n",
    "        # If pasal is too long, split into multiple chunks\n",
    "        if word_count > max_words:\n",
    "            chunks = text_splitter.create_documents([pasal_text])\n",
    "            for j, chunk in enumerate(chunks):\n",
    "                chunk.metadata = {\n",
    "                    \"bab_nomor\": current_bab,\n",
    "                    \"bab_judul\": current_bab_title,\n",
    "                    \"pasal_nomor\": pasal_nomor,\n",
    "                    \"chunk\": f\"{j+1}/{len(chunks)}\",\n",
    "                    \"source\": \"UU No. 13 Tahun 2003\",\n",
    "                    \"word_count\": len(chunk.page_content.split()),\n",
    "                    \"full_reference\": f\"{current_bab} {current_bab_title} - {pasal_nomor} (Bagian {j+1}/{len(chunks)})\"\n",
    "                }\n",
    "                documents.append(chunk)\n",
    "        else:\n",
    "            # If pasal is within desired size range, no need to split\n",
    "            doc = Document(\n",
    "                page_content=pasal_text,\n",
    "                metadata={\n",
    "                    \"bab_nomor\": current_bab,\n",
    "                    \"bab_judul\": current_bab_title,\n",
    "                    \"pasal_nomor\": pasal_nomor,\n",
    "                    \"chunk\": \"1/1\",\n",
    "                    \"source\": \"UU No. 13 Tahun 2003\",\n",
    "                    \"word_count\": word_count,\n",
    "                    \"full_reference\": f\"{current_bab} {current_bab_title} - {pasal_nomor}\"\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "    \n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_uu_ketenagakerjaan(text):\n",
    "    \"\"\"Proses teks UU dan ekstrak struktur bab, pasal, dan ayat dengan lebih akurat\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    # Preprocessing untuk normalisasi format\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalisasi whitespace\n",
    "    text = re.sub(r'(?<!\\n)BAB\\s+([IVXLCDM]+)', r'\\nBAB \\1', text)  # Pastikan BAB ada di baris baru\n",
    "    text = re.sub(r'(?<!\\n)Pasal\\s+(\\d+)', r'\\nPasal \\1', text)  # Pastikan Pasal ada di baris baru\n",
    "    \n",
    "    # Pola untuk deteksi struktur\n",
    "    bab_pattern = r'BAB\\s+([IVXLCDM]+)\\s*[-â€”]*\\s*([A-Z][A-Z\\s,/.&-]+)'\n",
    "    pasal_pattern = r'Pasal\\s+(\\d+)\\s*'\n",
    "    ayat_pattern = r'\\((\\d+)\\)\\s*(.*?)(?=\\(\\d+\\)|Pasal\\s+\\d+|BAB\\s+[IVXLCDM]+|$)'\n",
    "    point_pattern = r'([a-z])\\.\\s*(.*?)(?=[a-z]\\.\\s*|$)'\n",
    "    \n",
    "    # Ekstraksi BAB\n",
    "    bab_matches = list(re.finditer(bab_pattern, text, re.DOTALL))\n",
    "    \n",
    "    for i, bab_match in enumerate(bab_matches):\n",
    "        bab_nomor = bab_match.group(1).strip()\n",
    "        bab_judul = bab_match.group(2).strip()\n",
    "        \n",
    "        # Tentukan rentang konten BAB\n",
    "        bab_start = bab_match.end()\n",
    "        bab_end = text.length if i == len(bab_matches) - 1 else bab_matches[i + 1].start()\n",
    "        bab_content = text[bab_start:bab_end]\n",
    "        \n",
    "        # Ekstraksi pasal dalam BAB\n",
    "        pasal_matches = list(re.finditer(pasal_pattern, bab_content))\n",
    "        \n",
    "        for j, pasal_match in enumerate(pasal_matches):\n",
    "            pasal_nomor = pasal_match.group(1).strip()\n",
    "            \n",
    "            # Tentukan rentang konten pasal\n",
    "            pasal_start = pasal_match.end()\n",
    "            pasal_end = bab_end if j == len(pasal_matches) - 1 else pasal_matches[j + 1].start()\n",
    "            pasal_content = bab_content[pasal_start:pasal_end].strip()\n",
    "            \n",
    "            # Deteksi apakah pasal memiliki ayat atau langsung konten\n",
    "            ayat_matches = list(re.finditer(ayat_pattern, pasal_content, re.DOTALL))\n",
    "            \n",
    "            if ayat_matches:\n",
    "                # Proses setiap ayat\n",
    "                for ayat_match in ayat_matches:\n",
    "                    ayat_nomor = ayat_match.group(1).strip()\n",
    "                    ayat_content = ayat_match.group(2).strip()\n",
    "                    \n",
    "                    # Cek apakah ayat memiliki point-point\n",
    "                    point_matches = list(re.finditer(point_pattern, ayat_content))\n",
    "                    \n",
    "                    if point_matches:\n",
    "                        # Proses setiap point dalam ayat\n",
    "                        for point_match in point_matches:\n",
    "                            point_label = point_match.group(1).strip()\n",
    "                            point_content = point_match.group(2).strip()\n",
    "                            \n",
    "                            doc = Document(\n",
    "                                page_content=point_content,\n",
    "                                metadata={\n",
    "                                    \"bab_nomor\": f\"BAB {bab_nomor}\",\n",
    "                                    \"bab_judul\": bab_judul,\n",
    "                                    \"pasal_nomor\": f\"Pasal {pasal_nomor}\",\n",
    "                                    \"ayat_nomor\": ayat_nomor,\n",
    "                                    \"point\": point_label,\n",
    "                                    \"source\": \"UU No. 13 Tahun 2003\",\n",
    "                                    \"full_reference\": f\"BAB {bab_nomor} {bab_judul} - Pasal {pasal_nomor} Ayat ({ayat_nomor}) Point {point_label}\"\n",
    "                                }\n",
    "                            )\n",
    "                            documents.append(doc)\n",
    "                    else:\n",
    "                        # Proses ayat tanpa point\n",
    "                        doc = Document(\n",
    "                            page_content=ayat_content,\n",
    "                            metadata={\n",
    "                                \"bab_nomor\": f\"BAB {bab_nomor}\",\n",
    "                                \"bab_judul\": bab_judul,\n",
    "                                \"pasal_nomor\": f\"Pasal {pasal_nomor}\",\n",
    "                                \"ayat_nomor\": ayat_nomor,\n",
    "                                \"source\": \"UU No. 13 Tahun 2003\",\n",
    "                                \"full_reference\": f\"BAB {bab_nomor} {bab_judul} - Pasal {pasal_nomor} Ayat ({ayat_nomor})\"\n",
    "                            }\n",
    "                        )\n",
    "                        documents.append(doc)\n",
    "            else:\n",
    "                # Proses pasal tanpa ayat (langsung konten)\n",
    "                # Cek untuk point-point\n",
    "                point_matches = list(re.finditer(point_pattern, pasal_content))\n",
    "                \n",
    "                if point_matches:\n",
    "                    for point_match in point_matches:\n",
    "                        point_label = point_match.group(1)\n",
    "                        point_content = point_match.group(2).strip()\n",
    "                        \n",
    "                        doc = Document(\n",
    "                            page_content=point_content,\n",
    "                            metadata={\n",
    "                                \"bab_nomor\": f\"BAB {bab_nomor}\",\n",
    "                                \"bab_judul\": bab_judul,\n",
    "                                \"pasal_nomor\": f\"Pasal {pasal_nomor}\",\n",
    "                                \"point\": point_label,\n",
    "                                \"source\": \"UU No. 13 Tahun 2003\",\n",
    "                                \"full_reference\": f\"BAB {bab_nomor} {bab_judul} - Pasal {pasal_nomor} Point {point_label}\"\n",
    "                            }\n",
    "                        )\n",
    "                        documents.append(doc)\n",
    "                else:\n",
    "                    # Pasal dengan konten langsung tanpa ayat atau point\n",
    "                    doc = Document(\n",
    "                        page_content=pasal_content,\n",
    "                        metadata={\n",
    "                            \"bab_nomor\": f\"BAB {bab_nomor}\",\n",
    "                            \"bab_judul\": bab_judul,\n",
    "                            \"pasal_nomor\": f\"Pasal {pasal_nomor}\",\n",
    "                            \"source\": \"UU No. 13 Tahun 2003\",\n",
    "                            \"full_reference\": f\"BAB {bab_nomor} {bab_judul} - Pasal {pasal_nomor}\"\n",
    "                        }\n",
    "                    )\n",
    "                    documents.append(doc)\n",
    "    \n",
    "    # Fallback jika dokumen kosong, gunakan text splitter standar\n",
    "    if len(documents) < 5:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "        documents = text_splitter.create_documents([text])\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_find_bab(text):\n",
    "    lines = text.splitlines()\n",
    "    print(len(lines))\n",
    "    for line in lines:\n",
    "        if \"BAB\" in line:\n",
    "            print(repr(line))\n",
    "debug_find_bab(preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = chunk_uu_with_recursive_splitter(preprocessed_text)\n",
    "print(f\"Total dokumen yang dihasilkan: {len(documents)}\")\n",
    "\n",
    "# documents1 = parse_uu_ketenagakerjaan(preprocessed_text)\n",
    "# print(f\"Total dokumen yang dihasilkan: {len(documents)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tampilkan contoh 3 dokumen pertama\n",
    "for i, doc in enumerate(documents[:100]):\n",
    "    print(f\"\\nDokumen #{i+1}:\")\n",
    "    print(f\"Content: {doc.page_content[:100]}...\")\n",
    "    print(f\"panjang konten: {len(doc.page_content)}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store saved to ./chroma_db\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x273dd603790>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_vector_store(documents, persist_dir: str = \"./chroma_db\") -> Chroma:\n",
    "    \"\"\"\n",
    "    Create and persist a vector store from documents\n",
    "    \n",
    "    Args:\n",
    "        documents: List of processed documents\n",
    "        persist_dir: Directory to persist the vector store\n",
    "        \n",
    "    Returns:\n",
    "        Chroma: Vector store instance\n",
    "    \"\"\"\n",
    "    # Initialize embeddings model\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    \n",
    "    # Create vector store\n",
    "    db = Chroma.from_documents(\n",
    "        documents=documents, \n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_dir\n",
    "    )\n",
    "    \n",
    "    # Persist to disk\n",
    "    db.persist()\n",
    "    print(f\"Vector store saved to {persist_dir}\")\n",
    "    \n",
    "    return db\n",
    "\n",
    "create_vector_store(documents, persist_dir=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Template Prompt untuk RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template prompt untuk retrieval yang menyertakan metadata\n",
    "prompt_template = \"\"\"\n",
    "        Kamu adalah asisten hukum yang ahli tentang regulasi ketenagakerjaan di Indonesia, khususnya UU No. 13 Tahun 2003.\n",
    "        Berdasarkan konteks berikut, jawablah pertanyaan dengan relevan, akurat, dan jelas.\n",
    "        Sertakan nomor pasal jika tersedia dalam konteks.\n",
    "        Jika informasi tidak tersedia dalam konteks, cukup jawab bahwa kamu tidak menemukan informasi yang relevan dalam regulasi.\n",
    "        Jika ada informasi yang tidak relevan, abaikan informasi tersebut.\n",
    "        Informasi di konteks merupakan bagian dari UU No. 13 Tahun 2003.\n",
    "        KONTEKS:\n",
    "        {context}\n",
    "\n",
    "\n",
    "        PERTANYAAN:\n",
    "        {question}\n",
    "\n",
    "        JAWABAN:\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Template prompt untuk RAG:\")\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Format Referensi Pasal untuk Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_reference(source_doc):\n",
    "    \"\"\"Format referensi pasal dari metadata\"\"\"\n",
    "    metadata = source_doc.metadata\n",
    "    # Periksa apakah metadata memiliki struktur yang diharapkan\n",
    "    if 'pasal_nomor' in metadata and 'bab_nomor' in metadata and 'bab_judul' in metadata:\n",
    "        reference = f\"[{metadata['pasal_nomor']} UU No. 13 Tahun 2003 {metadata['bab_nomor']} - {metadata['bab_judul']}]\"\n",
    "        if 'ayat_nomor' in metadata and metadata['ayat_nomor'] != \"semua\":\n",
    "            reference += f\" Ayat ({metadata['ayat_nomor']})\"\n",
    "        return reference\n",
    "    else:\n",
    "        # Fallback jika metadata tidak lengkap\n",
    "        return \"[UU No. 13 Tahun 2003]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uji fungsi format_reference dengan dokumen contoh\n",
    "print(\"Contoh format referensi:\")\n",
    "print(format_reference(documents[0]))\n",
    "print(format_reference(documents[10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Setup QA System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_chain(vectorstore, model_name=\"gpt-3.5-turbo\", temperature=0):\n",
    "    \"\"\"\n",
    "    Build a RAG chain using the latest LangChain paradigm (LCEL)\n",
    "    \n",
    "    Args:\n",
    "        vectorstore: Vector store to use for retrieval\n",
    "        model_name: Name of the model to use\n",
    "        temperature: Temperature setting for the model\n",
    "        \n",
    "    Returns:\n",
    "        Chain: Query processing chain\n",
    "    \"\"\"\n",
    "    # Initialize LLM\n",
    "    llm = ChatOpenAI(temperature=temperature, model_name=model_name)\n",
    "    \n",
    "    # Create retriever\n",
    "    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 7})\n",
    "    \n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "    \n",
    "    # Build the chain with LCEL pattern (LangChain Expression Language)\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs_with_references, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ollama_rag_chain(vectorstore, model_name=\"mistral\"):\n",
    "    \"\"\"\n",
    "    Build a RAG chain using Ollama models\n",
    "    \n",
    "    Args:\n",
    "        vectorstore: Vector store to use for retrieval\n",
    "        model_name: Name of the Ollama model to use\n",
    "        \n",
    "    Returns:\n",
    "        Chain: Query processing chain\n",
    "    \"\"\"\n",
    "    # Initialize Ollama LLM\n",
    "    llm = Ollama(model=model_name, temperature=0)\n",
    "    \n",
    "    # Create retriever\n",
    "    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 7})\n",
    "    \n",
    "\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "    \n",
    "    # Build the chain with LCEL pattern\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs_with_references, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build RAG chains\n",
    "openai_chain = build_rag_chain(db)\n",
    "mistral_chain = build_ollama_rag_chain(db, model_name=\"mistral\")\n",
    "llama_chain = build_ollama_rag_chain(db, model_name=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def ask_question(b):\n",
    "    with out:\n",
    "        clear_output()\n",
    "        query = text.value.strip()\n",
    "\n",
    "        if query == \"\":\n",
    "            print(\"Please enter a question\")\n",
    "            return\n",
    "\n",
    "        print(\"Searching for an answer...\")\n",
    "\n",
    "        # Choose which chain to use (OpenAI or Ollama)\n",
    "        response = mistral_chain.invoke(query)  # Change to openai_chain if preferred\n",
    "\n",
    "        # Display answer\n",
    "        print(f\"Answer:\\n{textwrap.fill(response, width=100)}\\n\")\n",
    "\n",
    "# Question input widget\n",
    "text = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Enter your question about UU Ketenagakerjaan',\n",
    "    description='Question:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='80%', height='80px')\n",
    ")\n",
    "\n",
    "# Button to trigger answer search\n",
    "button = widgets.Button(description=\"Ask\")\n",
    "button.on_click(ask_question)\n",
    "\n",
    "# Output area\n",
    "out = widgets.Output()\n",
    "\n",
    "# Display all widgets\n",
    "display(text, button, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template= prompt_template\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "# Inisialisasi retriever dan LLM\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 7})\n",
    "\n",
    "# Fungsi untuk menyisipkan referensi dalam konteks\n",
    "def format_docs_with_references(docs):\n",
    "    formatted = \"\"\n",
    "    for doc in docs:\n",
    "        ref = doc.metadata.get(\"full_reference\", \"Tidak diketahui\")\n",
    "        formatted += f\"[{ref}]\\n{doc.page_content}\\n\\n\"\n",
    "    return formatted\n",
    "\n",
    "\n",
    "\n",
    "# Melakukan query\n",
    "query = \"bagaimana ketentuan mengenai jam kerja lembur dalam UU No. 13 Tahun 2003?\"\n",
    "# result = db.similarity_search(query, k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi model Mistral lokal via Ollama\n",
    "llm_mistral = Ollama(model=\"mistral\", temperature=0)\n",
    "llm_llama = Ollama(model=\"llama3.2\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain OpenAI GPT\n",
    "qa_chain_openai = LLMChain(llm=llm, prompt=custom_prompt_template)\n",
    "\n",
    "# Chain Mistral Ollama\n",
    "qa_chain_mistral = LLMChain(llm=llm_mistral, prompt=custom_prompt_template)\n",
    "\n",
    "# Chain Llama Ollama\n",
    "qa_chain_llama = LLMChain(llm=llm_llama, prompt=custom_prompt_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(query)\n",
    "formatted_context = format_docs_with_references(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Jalankan GPT\n",
    "# response_openai = qa_chain_openai.run({\n",
    "#     \"context\": formatted_context,\n",
    "#     \"question\": query\n",
    "# })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Jalankan Mistral\n",
    "# response_mistral = qa_chain_mistral.run({\n",
    "#     \"context\": formatted_context,\n",
    "#     \"question\": query\n",
    "# })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"=== Jawaban dari GPT-3.5 (OpenAI) ===\")\n",
    "# print(response_openai)\n",
    "\n",
    "# print(\"\\n=== Jawaban dari Mistral (Ollama) ===\")\n",
    "# print(response_mistral)\n",
    "\n",
    "# print(\"\\n=== Referensi yang digunakan ===\")\n",
    "# for doc in docs:\n",
    "#     print(f\"- {doc.metadata['full_reference']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Widget Interaktif untuk Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment baris ini jika ingin menggunakan widget interaktif di Jupyter Notebook\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display, clear_output\n",
    "import textwrap  # pastikan sudah mengimpor textwrap jika digunakan\n",
    "\n",
    "# Fungsi utama untuk menangani pertanyaan dari user\n",
    "def ask_question(b):\n",
    "    with out:\n",
    "        clear_output()\n",
    "        query = text.value.strip()\n",
    "\n",
    "        if query == \"\":\n",
    "            print(\"Mohon masukkan pertanyaan\")\n",
    "            return\n",
    "\n",
    "        print(\"Mencari jawaban...\")\n",
    "\n",
    "        # Ambil dokumen relevan dari retriever\n",
    "        docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "        # Format dokumen menjadi konteks beserta referensinya\n",
    "        formatted_context = format_docs_with_references(docs)\n",
    "\n",
    "        # Menjalankan model Mistral/Ollama dengan konteks dan pertanyaan\n",
    "        response_llama = qa_chain_mistral.run({\n",
    "            \"context\": formatted_context,\n",
    "            \"question\": query\n",
    "        })\n",
    "\n",
    "        # Tampilkan jawaban\n",
    "        print(f\"Jawaban dari Mistral (Ollama):\\n{textwrap.fill(response_llama, width=100)}\\n\")\n",
    "\n",
    "        # Tampilkan referensi dokumen yang digunakan\n",
    "        print(\"\\n=== Referensi yang digunakan ===\")\n",
    "        print(f\"{textwrap.fill(formatted_context, width=100)}\\n\")\n",
    "        for doc in docs:\n",
    "            wrapped_content = textwrap.fill(doc.metadata['full_reference'], width=100)\n",
    "            print(f\"- {wrapped_content}\")\n",
    "\n",
    "# Widget input pertanyaan\n",
    "text = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Masukkan pertanyaan Anda tentang UU Ketenagakerjaan',\n",
    "    description='Pertanyaan:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='80%', height='80px')\n",
    ")\n",
    "\n",
    "# Tombol untuk memicu pencarian jawaban\n",
    "button = widgets.Button(description=\"Tanya\")\n",
    "button.on_click(ask_question)\n",
    "\n",
    "# Output area\n",
    "out = widgets.Output()\n",
    "\n",
    "# Tampilkan semua widget\n",
    "display(text, button, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
